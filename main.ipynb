{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chess AI\n",
    "construct $f(p)$ as a 3 layer deep 2048 units wide artificial neural network\\\n",
    "for each move, $f(p) = \\max\\limits_{p\\rightarrow p_0} - f(p_0)$\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chess\n",
    "import chess.pgn\n",
    "import time\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "mp.set_start_method(\"spawn\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LAMBDA = 10.0\n",
    "EPOCHS = 10\n",
    "KAPPA = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "1. Players will choose an optimal or near-optimal move. This means that for two position in succession \n",
    "$p \\rightarrow q$ observed in the game, we will have $f(p) = -f(q)$\n",
    "2. For the same reason above, going from $p$ not to $q$, but to a random position $r$, we must have $f(r) > f(q)$ because the random position is better for the next player and worse for the player that made the move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/dimitrioskourtikakis/gm-games-chesscom/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"dimitrioskourtikakis/gm-games-chesscom\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4812it [02:23, 33.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "chessGames = pandas.read_csv(path+\"/GM_games_dataset.csv\", chunksize=1000)\n",
    "# copy the \"pgn\" column to a pgn file\n",
    "for i, chunk in enumerate(tqdm(chessGames)):\n",
    "    with open(f\"./data/chessGame{i}.pgn\", \"w\") as f:\n",
    "        for pgn in chunk['pgn']:\n",
    "            f.write(pgn + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index = {\n",
    "    'P': 0,  'N': 1,  'B': 2,  'R': 3,  'Q': 4,  'K': 5,  # White pieces\n",
    "    'p': 6,  'n': 7,  'b': 8,  'r': 9,  'q': 10, 'k': 11,  # Black pieces\n",
    "}\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, pgn_file, device='cpu'):\n",
    "        self.device = device\n",
    "        self.moves_data = []\n",
    "        \n",
    "        # 读取PGN文件中的游戏并生成每一步的p, q, r\n",
    "        with open(pgn_file) as f:\n",
    "            while game:=chess.pgn.read_game(f):\n",
    "                moves = list(game.mainline_moves())\n",
    "                board = game.board()\n",
    "                for move in moves[:-1]:\n",
    "                    p = self.board2vec(board)\n",
    "                    legal_moves = list(board.legal_moves)\n",
    "                    pseudo_move = random.choice(legal_moves)\n",
    "                    board.push(pseudo_move)\n",
    "                    r = self.board2vec(board)\n",
    "                    board.pop()\n",
    "                    board.push(move)\n",
    "                    q = self.board2vec(board)\n",
    "                    self.moves_data.append((p, q, r))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.moves_data)\n",
    "    \n",
    "    def board2vec(self, board):\n",
    "        # 使用 NumPy 进行初始处理\n",
    "        vec = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "        for square in chess.SQUARES:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece is not None:\n",
    "                piece_index = piece_to_index[piece.symbol()]\n",
    "                row, col = divmod(square, 8)\n",
    "                vec[piece_index, row, col] = 1\n",
    "        \n",
    "        # 将 NumPy 数组转换为 PyTorch 张量并移动到设备上\n",
    "        vec = torch.tensor(vec, dtype=torch.float32).to(self.device)\n",
    "        return vec\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.moves_data[idx]\n",
    "\n",
    "class ChessValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessValueNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(12, 64, kernel_size=3, padding=1),  # [12, 8, 8] -> [64, 8, 8]\n",
    "            nn.BatchNorm2d(64),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # [64, 8, 8] -> [128, 8, 8]\n",
    "            nn.BatchNorm2d(128),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # [128, 8, 8] -> [128, 4, 4]\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # [128, 4, 4] -> [256, 4, 4]\n",
    "            nn.BatchNorm2d(256),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)  # [256, 4, 4] -> [256, 2, 2]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 * 2 * 2, 512),  # [1, 256*2*2] -> [1, 512]\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout\n",
    "            nn.Linear(512, 256),  # [1, 512] -> [1, 256]\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout\n",
    "            nn.Linear(256, 1),  # 输出标量\n",
    "            nn.Tanh()  # 限制在 [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平成 [Batch Size, Features]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def objective_function(model, p, q, r, kappa=10.0):\n",
    "    # Forward pass: Compute scores for p, q, and r\n",
    "    f_p = model(p).squeeze()  # Score for p\n",
    "    f_q = model(q).squeeze()  # Score for q\n",
    "    f_r = model(r).squeeze()  # Score for r\n",
    "    # Loss components\n",
    "    # Loss A: Ensure f(q) > f(r) (optimal move vs random move)\n",
    "    loss_a = -torch.log(F.sigmoid(f_q - f_r)).mean()\n",
    "    # Loss B: Ensure f(p) + f(q) close to zero (soft equality constraint)\n",
    "    loss_b = -torch.log(F.sigmoid(kappa * (f_p + f_q))).mean()\n",
    "    # Loss C: Ensure -f(p) - f(q) close to zero (soft equality constraint)\n",
    "    loss_c = -torch.log(F.sigmoid(-kappa * (f_p + f_q))).mean()\n",
    "    # Total loss: Combine all components\n",
    "    total_loss = loss_a + loss_b + loss_c\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def lr_lambda(current_epoch):\n",
    "    current_time = time.time()  # 当前时间戳\n",
    "    elapsed_time = current_time - t0\n",
    "    return math.exp(-elapsed_time / 86400)\n",
    "\n",
    "model = ChessValueNetwork().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03, weight_decay=LAMBDA)\n",
    "t0 = time.time()\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "writer = SummaryWriter(log_dir=\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset for ./data/chessGame0.pgn\n"
     ]
    }
   ],
   "source": [
    "pgn_file = \"./data/chessGame0.pgn\"\n",
    "print(\"creating dataset for \" + pgn_file)\n",
    "chess_dataset = ChessDataset(pgn_file, device=device)\n",
    "dataloader = DataLoader(chess_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board2vec(self, board):\n",
    "        # 使用 NumPy 进行初始处理\n",
    "        vec = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "        for square in chess.SQUARES:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece is not None:\n",
    "                piece_index = piece_to_index[piece.symbol()]\n",
    "                row, col = divmod(square, 8)\n",
    "                vec[piece_index, row, col] = 1\n",
    "        \n",
    "        # 将 NumPy 数组转换为 PyTorch 张量并移动到设备上\n",
    "        vec = torch.tensor(vec, dtype=torch.float32).to(self.device)\n",
    "        return vec\n",
    "# checkmate 1-0\n",
    "strio = \n",
    "board = chess.Board()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN File [./data/chessGame0.pgn], Batch [0], Loss: 1.9773, lr: 0.02997369765696147\n",
      "tensor([[0.1016]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [1000], Loss: 2.0794, lr: 0.02997123070268871\n",
      "tensor([[-1.3438e-25]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [2000], Loss: 2.0794, lr: 0.02996876506862384\n",
      "tensor([[-0.0008]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [3000], Loss: 2.0794, lr: 0.029966291939176437\n",
      "tensor([[-0.0005]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [4000], Loss: 2.0794, lr: 0.029963831863886736\n",
      "tensor([[-0.0001]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [5000], Loss: 2.0794, lr: 0.029961365821482466\n",
      "tensor([[-0.0014]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [6000], Loss: 2.0794, lr: 0.029958903978094475\n",
      "tensor([[0.0001]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [7000], Loss: 2.0795, lr: 0.029956438404165658\n",
      "tensor([[3.0265e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [8000], Loss: 2.0795, lr: 0.02995392193665008\n",
      "tensor([[-0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [9000], Loss: 2.0794, lr: 0.0299514322683445\n",
      "tensor([[0.0005]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [10000], Loss: 2.0794, lr: 0.02994897793801407\n",
      "tensor([[-0.0012]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [11000], Loss: 2.0794, lr: 0.029946504451912232\n",
      "tensor([[-0.0008]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [12000], Loss: 2.0795, lr: 0.029944062560175573\n",
      "tensor([[-0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [13000], Loss: 2.0794, lr: 0.029941588673799115\n",
      "tensor([[0.0001]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [14000], Loss: 2.0794, lr: 0.02993911136718642\n",
      "tensor([[0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [15000], Loss: 2.0794, lr: 0.029936639920401006\n",
      "tensor([[-0.0015]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [16000], Loss: 2.0795, lr: 0.029934147680396032\n",
      "tensor([[-0.0005]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [17000], Loss: 2.0794, lr: 0.029931677245976684\n",
      "tensor([[0.0001]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [18000], Loss: 2.0795, lr: 0.029929190867831964\n",
      "tensor([[-0.0011]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [19000], Loss: 2.0795, lr: 0.02992673596805413\n",
      "tensor([[-0.0004]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [20000], Loss: 2.0794, lr: 0.029924286023000082\n",
      "tensor([[0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [21000], Loss: 2.0795, lr: 0.029921814447148986\n",
      "tensor([[-8.7078e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [22000], Loss: 2.0794, lr: 0.02991935609483439\n",
      "tensor([[-0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [23000], Loss: 2.0794, lr: 0.02991689465080296\n",
      "tensor([[0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [24000], Loss: 2.0795, lr: 0.02991440256205052\n",
      "tensor([[-4.8744e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [25000], Loss: 2.0794, lr: 0.029911911984957663\n",
      "tensor([[0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [26000], Loss: 2.0794, lr: 0.02990943813603114\n",
      "tensor([[0.0004]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [27000], Loss: 2.0794, lr: 0.029906970128331756\n",
      "tensor([[0.0008]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [28000], Loss: 2.0794, lr: 0.02990449656029433\n",
      "tensor([[-0.0005]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [29000], Loss: 2.0794, lr: 0.029902039401332275\n",
      "tensor([[-0.0008]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [30000], Loss: 2.0795, lr: 0.02989957559658888\n",
      "tensor([[0.0013]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [31000], Loss: 2.0794, lr: 0.029897110683922905\n",
      "tensor([[-0.0006]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [32000], Loss: 2.0795, lr: 0.029894652022228248\n",
      "tensor([[-0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [33000], Loss: 2.0794, lr: 0.02989218051159602\n",
      "tensor([[0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [34000], Loss: 2.0795, lr: 0.029889672331776692\n",
      "tensor([[2.7220e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [35000], Loss: 2.0794, lr: 0.02988721208034134\n",
      "tensor([[-0.0015]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [36000], Loss: 2.0795, lr: 0.029884737726002794\n",
      "tensor([[5.3515e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [37000], Loss: 2.0794, lr: 0.02988226487541491\n",
      "tensor([[-0.0011]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [38000], Loss: 2.0794, lr: 0.029879813691019965\n",
      "tensor([[-0.0006]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [39000], Loss: 2.0794, lr: 0.02987735822115494\n",
      "tensor([[8.3406e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [40000], Loss: 2.0794, lr: 0.029874895514438723\n",
      "tensor([[-0.0009]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [41000], Loss: 2.0794, lr: 0.02987242934800629\n",
      "tensor([[0.0006]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [42000], Loss: 2.0794, lr: 0.029869966412640343\n",
      "tensor([[0.0005]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [43000], Loss: 2.0795, lr: 0.02986750752862951\n",
      "tensor([[0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [44000], Loss: 2.0794, lr: 0.02986500260306886\n",
      "tensor([[-0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [45000], Loss: 2.0795, lr: 0.0298625298540764\n",
      "tensor([[4.3895e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [46000], Loss: 2.0794, lr: 0.029860065228440962\n",
      "tensor([[0.0012]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [47000], Loss: 2.0794, lr: 0.02985760933767062\n",
      "tensor([[-0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [48000], Loss: 2.0794, lr: 0.029855147173301056\n",
      "tensor([[-0.0007]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [49000], Loss: 2.0795, lr: 0.02985267135711234\n",
      "tensor([[0.0004]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [50000], Loss: 2.0795, lr: 0.029850210020614287\n",
      "tensor([[-0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [51000], Loss: 2.0795, lr: 0.02984773654306523\n",
      "tensor([[0.0010]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [52000], Loss: 2.0794, lr: 0.02984529895317017\n",
      "tensor([[0.0006]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [53000], Loss: 2.0794, lr: 0.029842842943621162\n",
      "tensor([[0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [54000], Loss: 2.0795, lr: 0.029840358797581592\n",
      "tensor([[-0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [55000], Loss: 2.0794, lr: 0.02983788445205113\n",
      "tensor([[-0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [56000], Loss: 2.0794, lr: 0.02983540172335196\n",
      "tensor([[2.3497e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [57000], Loss: 2.0795, lr: 0.029832932042416854\n",
      "tensor([[0.0004]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [58000], Loss: 2.0794, lr: 0.029830478269746198\n",
      "tensor([[0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [59000], Loss: 2.0795, lr: 0.029828010770296207\n",
      "tensor([[0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [60000], Loss: 2.0794, lr: 0.029825546179257544\n",
      "tensor([[-0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [61000], Loss: 2.0794, lr: 0.029823082128203848\n",
      "tensor([[0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [62000], Loss: 2.0795, lr: 0.029820602500611193\n",
      "tensor([[-0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [63000], Loss: 2.0794, lr: 0.02981815645876566\n",
      "tensor([[0.0004]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [64000], Loss: 2.0794, lr: 0.029815665690211733\n",
      "tensor([[-0.0008]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [65000], Loss: 2.0794, lr: 0.0298132189628361\n",
      "tensor([[-0.0005]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [66000], Loss: 2.0795, lr: 0.02981074721700967\n",
      "tensor([[0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [67000], Loss: 2.0794, lr: 0.029808318057458722\n",
      "tensor([[0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [68000], Loss: 2.0794, lr: 0.029805870442946393\n",
      "tensor([[0.0013]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [69000], Loss: 2.0795, lr: 0.029803388902382693\n",
      "tensor([[0.0006]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [70000], Loss: 2.0795, lr: 0.029800913619757487\n",
      "tensor([[-2.6683e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [71000], Loss: 2.0794, lr: 0.0297984322955224\n",
      "tensor([[0.0005]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [72000], Loss: 2.0794, lr: 0.029795931739745627\n",
      "tensor([[-0.0007]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [73000], Loss: 2.0795, lr: 0.029793460913899526\n",
      "tensor([[0.0011]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [74000], Loss: 2.0794, lr: 0.029790968246637045\n",
      "tensor([[-0.0002]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [75000], Loss: 2.0794, lr: 0.029788520935038632\n",
      "tensor([[0.0008]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [76000], Loss: 2.0794, lr: 0.029786017903092755\n",
      "tensor([[-6.5204e-05]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [77000], Loss: 2.0795, lr: 0.029783548940906002\n",
      "tensor([[-0.0003]], device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "PGN File [./data/chessGame0.pgn], Batch [78000], Loss: 2.0794, lr: 0.029781085496974474\n",
      "tensor([[0.0010]], device='cuda:0', grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "model.train()\n",
    "for batch_idx, (p, q, r) in enumerate(dataloader):\n",
    "    p, q, r = p.to(device), q.to(device), r.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = objective_function(model, p, q, r, KAPPA)\n",
    "    loss.backward()\n",
    "    # 梯度裁剪\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if batch_idx % 1000 == 0:\n",
    "        print(f\"PGN File [{pgn_file}], Batch [{batch_idx}], Loss: {loss.item():.4f}, lr: {scheduler.get_last_lr()[0]}\")\n",
    "        print(model(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset for ./data/chessGame0.pgn\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [0], Loss: 2.3184, lr: 0.029986162701996198\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [1000], Loss: 2.0794, lr: 0.02998372358738248\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [2000], Loss: 2.0794, lr: 0.029981299299038686\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [3000], Loss: 2.0795, lr: 0.02997886849349813\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [4000], Loss: 2.0795, lr: 0.029976446743442426\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [5000], Loss: 2.0795, lr: 0.029974031373603882\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [6000], Loss: 2.0794, lr: 0.0299715957369514\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [7000], Loss: 2.0795, lr: 0.02996917323707462\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [8000], Loss: 2.0794, lr: 0.029966716951123983\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [9000], Loss: 2.0795, lr: 0.029964303237438192\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [10000], Loss: 2.0794, lr: 0.029961879678616785\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [11000], Loss: 2.0795, lr: 0.02995943254446658\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [12000], Loss: 2.0794, lr: 0.029956999337213883\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [13000], Loss: 2.0794, lr: 0.02995452992182996\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [14000], Loss: 2.0795, lr: 0.029952114532584364\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [15000], Loss: 2.0794, lr: 0.02994970037398192\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [16000], Loss: 2.0795, lr: 0.0299472485211857\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [17000], Loss: 2.0794, lr: 0.029944832811364744\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [18000], Loss: 2.0794, lr: 0.0299424151399698\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [19000], Loss: 2.0795, lr: 0.02993995500944336\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [20000], Loss: 2.0794, lr: 0.02993752113820741\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [21000], Loss: 2.0795, lr: 0.02993509725295334\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [22000], Loss: 2.0794, lr: 0.029932680391041154\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [23000], Loss: 2.0795, lr: 0.02993025587184477\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [24000], Loss: 2.0794, lr: 0.02992783145232505\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [25000], Loss: 2.0794, lr: 0.02992542605277411\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [26000], Loss: 2.0795, lr: 0.029923004131464377\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [27000], Loss: 2.0794, lr: 0.029920582011339122\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [28000], Loss: 2.0794, lr: 0.029918167887968545\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [29000], Loss: 2.0794, lr: 0.02991574654416961\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [30000], Loss: 2.0794, lr: 0.02991332778436387\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [31000], Loss: 2.0794, lr: 0.029910912676250657\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [32000], Loss: 2.0794, lr: 0.029908493517776292\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [33000], Loss: 2.0795, lr: 0.029906064356847437\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [34000], Loss: 2.0794, lr: 0.029903624885994465\n",
      "Epoch [1/10], PGN File [./data/chessGame0.pgn], Batch [35000], Loss: 2.0794, lr: 0.029901210921426016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     35\u001b[0m pgn_files \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/chessGame\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pgn\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4812\u001b[39m)]\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgn_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKAPPA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, objective_function, pgn_files, device, EPOCHS, KAPPA, writer)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 梯度裁剪\u001b[39;00m\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:535\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    533\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_grads, device_params, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m         device_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m    536\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    540\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练代码\n",
    "def train_model(model, optimizer, scheduler, objective_function, pgn_files, device, EPOCHS, KAPPA, writer):\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for pgn_file in pgn_files:\n",
    "            # 为每个PGN文件创建数据集和数据加载器\n",
    "            print(\"creating dataset for \" + pgn_file)\n",
    "            chess_dataset = ChessDataset(pgn_file, device=device)\n",
    "            dataloader = DataLoader(chess_dataset, batch_size=1, shuffle=True)\n",
    "            \n",
    "            for batch_idx, (p, q, r) in enumerate(dataloader):\n",
    "                p, q, r = p.to(device), q.to(device), r.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = objective_function(model, p, q, r, KAPPA)\n",
    "                loss.backward()\n",
    "                # 梯度裁剪\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % 1000 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{EPOCHS}], PGN File [{pgn_file}], Batch [{batch_idx}], Loss: {loss.item():.4f}, lr: {scheduler.get_last_lr()[0]}\")\n",
    "            writer.add_text(\"PGN Files\", str(pgn_files), epoch)\n",
    "        \n",
    "        # 记录损失,学习率,PGNfile到 TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", total_loss, epoch)\n",
    "        writer.add_scalar(\"Learning Rate\", scheduler.get_last_lr()[0], epoch)\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "pgn_files = [f\"./data/chessGame{i}.pgn\" for i in range(4812)]\n",
    "train_model(model, optimizer, scheduler, objective_function, pgn_files, device, EPOCHS, KAPPA, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
